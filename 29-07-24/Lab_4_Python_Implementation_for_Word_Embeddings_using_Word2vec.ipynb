{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQcyGmWYF27a"
      },
      "source": [
        "### Lab 4 : Python Implementation of Word Embeddings using word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxUQPzFSF27c"
      },
      "source": [
        "The gensim library needs scipy 1.12.0 so first you need to install this using the following command\n",
        "\n",
        "`pip install scipy==1.12`\n",
        "\n",
        "`pip install gensim`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evDw9u9YF27d",
        "outputId": "22110868-d9c3-45cb-978b-204309f93d92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Import necessary Libraries\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import download\n",
        "download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQRGFyEfF27e",
        "outputId": "53249836-e510-42b6-8720-593d6805627c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['natural', 'language', 'processing', 'is', 'fun', '.'],\n",
              " ['language', 'models', 'are', 'improving', 'every', 'day', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Example sentences\n",
        "sentences = [\n",
        "    \"Natural Language Processing is fun.\",\n",
        "    \"Language models are improving every day.\"\n",
        "]\n",
        "\n",
        "# Tokenize sentences\n",
        "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "tokenized_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sANyzlAGF27f",
        "outputId": "a128ebc9-e062-4a38-849a-60d5b804e5db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Vector for 'language': [-0.14233617  0.12917745  0.17945977 -0.10030856 -0.07526743]\n"
          ]
        }
      ],
      "source": [
        "# Train the Word2Vec model\n",
        "model = Word2Vec(sentences=tokenized_sentences, vector_size=5, window=5, min_count=1, workers=4, sg=0)\n",
        "# Here sg=0 means the model will use Continuous bag of words architecture and if sg=1 then it will use Skip-gram Model\n",
        "\n",
        "# Get word vectors\n",
        "word_vectors = model.wv\n",
        "print(\"Word Vector for 'language':\", word_vectors['language'])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dataScience",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}